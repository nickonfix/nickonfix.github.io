---
layout: about
title: About
permalink: /
subtitle: > 
    <!-- Research Assistant @ <a href='https://www.tifr.res.in/~abhishek.sinha/group.html'> TIFR </a> <BR> -->
    <!-- Previously: M.Sc, CS @ <a href='https://www.cmi.ac.in/'> CMI </a> and B.Tech, ICT @ <a href='https://ahduni.edu.in/academics/schools-centres/school-of-engineering-and-applied-science/'> SEAS, AU </a> -->
profile:
  align: right
  image: no-smile.png
  # adDr.ess: >
  #   <p>555 your office number</p>
  #   <p>123 your adDr.ess street</p>
  #   <p>Your City, State 12345</p>

news: true  # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
posts: false
---

I am a Machine Learning Engineer at <a href="https://www.linkedin.com/company/atheropoint" target="_blank">AtheroPoint</a>, where I work on developing **novel transformer-based architectures for biomedical and imaging applications**. My work focuses on advancing deep learning approaches for data-efficient model design, representation learning, and transformer optimization.  

I conduct my research under the guidance of <a href="https://scholar.google.com/citations?user=AKJK7UQAAAAJ&hl=en" target="_blank">Dr. Jasjit Singh Suri</a> and <a href="https://scholar.google.co.in/citations?user=cppq2DcAAAAJ&hl=en&oi=ao" target="_blank">Prof. Luca Saba</a>, whose mentorship has been instrumental in shaping my interest in transformer architectures and their extensions to domain-specific modeling.  

My broader research interests span **transformers**, **natural language processing (NLP)**, **multimodal learning**, and **representation learning**. I am particularly fascinated by how large-scale attention mechanisms can be adapted to improve interpretability, generalization, and robustness in real-world datasets.  

I am currently preparing for **MS/PhD applications**, aiming to contribute to research at the intersection of deep learning innovation and practical deployment, and to collaborate with researchers working on **transformers, NLP, and scalable model architectures**.

Previously, I completed my **Bachelor of Technology** from <a href="https://bvcoend.ac.in/" target="_blank">Bharati Vidyapeeth College of Engineering</a>, Delhi, where I was mentored by <a href="https://scholar.google.com/citations?user=4IC-Sw8AAAAJ&hl=en&oi=ao" target="_blank">Dr. Arun K. Dubey</a> and <a href="https://scholar.google.co.in/citations?hl=en&user=mFlMAqcAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Dr. Rubbena Vohra</a>. My undergraduate research laid the foundation for my ongoing interests in **transformers**, **NLP**, and **representation learning**.  


<!-- As an undergrad, I was a research intern at IIT-Gandhinagar in 2017 and a summer student at Institute of Mathematical Sciences, Chennai in 2018. I completed my BTech project at CMI where I was advised by Dr. [Partha Mukhopadhyay](https://www.cmi.ac.in/~partham/), and my masterâ€™s thesis with Dr. [Navin Kashyap](https://ece.iisc.ac.in/~nkashyap/) at Indian Institute of Science, Bangalore. -->

<!-- My CV can be found [here]({{site.url}}/assets/pdf/Resume_Ativ.pdf){:target="_blank"}. -->

<!-- Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com). You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder.

Put your adDr.ess / P.O. box / other info right below your picture. You can also disable any these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](http://fortawesome.github.io/Font-Awesome/) and [Academicons](https://jpswalsh.github.io/academicons/), like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->
