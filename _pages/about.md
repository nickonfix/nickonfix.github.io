---
layout: about
title: About
permalink: /
subtitle: 
profile:
  align: right
  image: no-smile.png
  # adDr.ess: >
  #   <p>555 your office number</p>
  #   <p>123 your adDr.ess street</p>
  #   <p>Your City, State 12345</p>

news: true  # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
posts: false
---

Hi, I'm a Research Intern at <a href="https://www.linkedin.com/company/atheropoint" target="_blank">AtheroPoint</a>, where I focus on developing <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)" target="_blank">transformer-based architectures</a> for <a href="https://en.wikipedia.org/wiki/Biomedical_engineering" target="_blank">biomedical</a> and <a href="https://en.wikipedia.org/wiki/Medical_imaging" target="_blank">imaging applications</a>. My work emphasizes data-efficient <a href="https://en.wikipedia.org/wiki/Deep_learning" target="_blank">deep learning</a> model design, <a href="https://en.wikipedia.org/wiki/Representation_learning" target="_blank">representation learning</a>, and transformer optimization.

I conduct my research under the guidance of <a href="https://scholar.google.com/citations?user=AKJK7UQAAAAJ&hl=en" target="_blank">Dr. Jasjit Singh Suri</a> and <a href="https://scholar.google.co.in/citations?user=cppq2DcAAAAJ&hl=en&oi=ao" target="_blank">Prof. Luca Saba</a>, whose mentorship has shaped my focus on domain-specific transformer architectures.

My broader interests span <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)" target="_blank">transformers</a>, <a href="https://en.wikipedia.org/wiki/Natural_language_processing" target="_blank">NLP</a>, <a href="https://en.wikipedia.org/wiki/Multimodal_learning" target="_blank">multimodal learning</a>, and <a href="https://en.wikipedia.org/wiki/Representation_learning" target="_blank">representation learning</a>. I am particularly fascinated by large-scale <a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)" target="_blank">attention mechanisms</a> and their potential to improve interpretability, generalization, and robustness in complex datasets.

I am preparing for MS/PhD applications to contribute to research at the intersection of deep learning and scalable model deployment. I aim to collaborate with leading researchers on transformers, NLP, and advanced model architectures, bridging theoretical insights and applied intelligence systems.

I earned my <a href="https://en.wikipedia.org/wiki/Bachelor_of_Technology" target="_blank">B.Tech</a> from <a href="https://bvcoend.ac.in/" target="_blank">Bharati Vidyapeeth College of Engineering</a>, Delhi, under the mentorship of <a href="https://scholar.google.com/citations?user=4IC-Sw8AAAAJ&hl=en&oi=ao" target="_blank">Dr. Arun K. Dubey</a> and <a href="https://scholar.google.co.in/citations?hl=en&user=mFlMAqcAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Dr. Rubbena Vohra</a>. My undergraduate research laid the foundation in transformers, NLP, and representation learning, guiding my current academic pursuits.

Other than research, I actively pursue <a href="https://en.wikipedia.org/wiki/Competitive_programming" target="_blank">competitive programming</a> to strengthen my <a href="https://en.wikipedia.org/wiki/Algorithm" target="_blank">algorithmic reasoning</a> and <a href="https://en.wikipedia.org/wiki/Computational_efficiency" target="_blank">computational efficiency</a>. This pursuit complements my research by enhancing precision in <a href="https://en.wikipedia.org/wiki/Problem_solving" target="_blank">problem-solving</a> and optimization of <a href="https://en.wikipedia.org/wiki/Deep_learning" target="_blank">deep learning</a> models.


