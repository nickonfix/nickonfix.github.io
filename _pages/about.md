---
layout: about
title: About
permalink: /
subtitle: 
profile:
  align: right
  image: no-smile.png
  # adDr.ess: >
  #   <p>555 your office number</p>
  #   <p>123 your adDr.ess street</p>
  #   <p>Your City, State 12345</p>

news: true  # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
posts: false
---
Hi, I’m a Research Intern at <a href="https://www.linkedin.com/company/atheropoint" target="_blank">AtheroPoint</a>, where I work on transformer-based architectures, embedded attention mechanisms, and data-efficient <a href="https://en.wikipedia.org/wiki/Deep_learning" target="_blank">deep learning</a> models for large-scale biomedical and imaging datasets. My research focuses on sequence modeling, <a href="https://en.wikipedia.org/wiki/Representation_learning" target="_blank">representation learning</a>, and attention-driven transformer variants methods that naturally extend to modern <a href="https://en.wikipedia.org/wiki/Natural_language_processing" target="_blank">NLP</a> systems.

Although my recent work is rooted in biomedical AI, the underlying techniques I use transformers, self-supervised learning, and attention mechanisms are fundamentally domain general. These are the same principles that drive today’s NLP systems, motivating my transition toward language modeling, multimodal understanding, and efficient transformer design.

My current research at AtheroPoint is supervised by
<a href="https://scholar.google.com/citations?user=AKJK7UQAAAAJ&hl=en" target="_blank">Dr. Jasjit Singh Suri</a> and
<a href="https://scholar.google.co.in/citations?user=cppq2DcAAAAJ&hl=en&oi=ao" target="_blank">Prof. Luca Saba</a>,
whose mentorship has strengthened my foundations in deep learning architecture design, attention models, and model optimization.

I completed my Bachelor of Technology at 
  <a href="https://bvcoend.ac.in/" target="_blank" rel="noopener noreferrer">
    Bharati Vidyapeeth College of Engineering, New Delhi
  </a>. where I worked under the mentorship of
<a href="https://scholar.google.com/citations?user=4IC-Sw8AAAAJ&hl=en&oi=ao" target="_blank">Dr. Arun K. Dubey</a> and
<a href="https://scholar.google.co.in/citations?hl=en&user=mFlMAqcAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Dr. Rubbena Vohra</a>.
Their early guidance introduced me to deep learning, transformers, and representation learning, shaping my interest in model generalization and sequence understanding.

I am currently preparing for Fall 2026 MS/PhD applications, with broad research interests in transformers, NLP, self-supervised learning, sequence modeling, and scalable model efficiency.

Outside research, I practice <a href="https://en.wikipedia.org/wiki/Competitive_programming" target="_blank">competitive programming</a> to strengthen my <a href="https://en.wikipedia.org/wiki/Algorithm" target="_blank">algorithmic reasoning</a> and <a href="https://en.wikipedia.org/wiki/Computational_efficiency" target="_blank">computational efficiency</a>, which supports my work on optimizing deep learning architectures.